{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping League of Legends mains subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas - Do document clustering by subreddits, get most important words for each cluster and each subreddit (n_clusters = # of roles in league)\n",
    "\n",
    "Try clustering by top all time, and top monthly \n",
    "\n",
    "See corellation between post activity and win percentage (scrape op.gg)\n",
    "\n",
    "Create network of people who post in multiple subreddits\n",
    "\n",
    "compare the average upvote ratio for each subreddit \n",
    "\n",
    "compare the videos to non-videos posted\n",
    "\n",
    "Spot shitposting - figure out how later\n",
    "\n",
    "Plot activity by date of release and by date of last update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of champions - to be used for subreddits URLs\n",
    "# There are some special cases, which will be explained next cell...\n",
    "\n",
    "mains = [\"Aatrox\",\"Ahri\",\"Akali\",\"Amumu\",\"Anivia\",\"Annie\",\"Ashe\",\"Aurelion_Sol_\",\n",
    "         \"Azir\",\"Bard\",\n",
    "\n",
    "\"Blitzcrank\",\n",
    "\n",
    "\"Brand\",\n",
    "\n",
    "\"Braum\",\n",
    "\n",
    "\"Caitlyn\",\n",
    "\n",
    "\"Camille\",\n",
    "\n",
    "\"Cassiopeia\",\n",
    "\n",
    "\"ChoGath\",\n",
    "\n",
    "\"Corki\",\n",
    "\n",
    "\"Darius\",\n",
    "\n",
    "\"Diana\",\n",
    "\n",
    "\"DrMundo\",\n",
    "\n",
    "\"Ekko\",\n",
    "\n",
    "\"Elise\",\n",
    "\n",
    "\"Evelynn\",\n",
    "\n",
    "\"Ezreal\",\n",
    "\n",
    "\"Fiddlesticks\",\n",
    "\n",
    "\"Fiora\",\n",
    "\n",
    "\"Fizz\",\n",
    "\n",
    "\"Galio\",\n",
    "\n",
    "\"Gangplank\",\n",
    "\n",
    "\"Garen\",\n",
    "\n",
    "\"Gnar\",\n",
    "\n",
    "\"Gragas\",\n",
    "\n",
    "\"Graves\",\n",
    "\n",
    "\"Hecarim\",\n",
    "\n",
    "\"Heimerdinger\",\n",
    "\n",
    "\"Irelia\",\n",
    "\n",
    "\"Ivern\",\n",
    "\n",
    "\"JarvanIV\",\n",
    "\n",
    "\"Jax\",\n",
    "\n",
    "\"Jayce\",\n",
    "\n",
    "\"Jhin\",\n",
    "\n",
    "\"Kalista\",\n",
    "\n",
    "\"Karma\",\n",
    "\n",
    "\"Karthus\",\n",
    "\n",
    "\"Kassadin\",\n",
    "\n",
    "\"Katarina\",\n",
    "\n",
    "\"Kayle\",\n",
    "\n",
    "\"Kayn\",\n",
    "\n",
    "\"Kennen\",\n",
    "\n",
    "\"KhaZix\",\n",
    "\n",
    "\n",
    "\"Kled\",\n",
    "\n",
    "\"KogMaw\",\n",
    "\n",
    "\"LeBlanc\",\n",
    "\n",
    "\"LeeSin\",\n",
    "\n",
    "\"Leona\",\n",
    "\n",
    "\"Lissandra\",\n",
    "\n",
    "\"Lucian\",\n",
    "\n",
    "\"Lulu\",\n",
    "\n",
    "\"Malphite\",\n",
    "\n",
    "\"Malzahar\",\n",
    "\n",
    "\"Maokai\",\n",
    "\n",
    "\"MasterYi\",\n",
    "\n",
    "\"MissFortune\",\n",
    "\n",
    "\"Mordekaiser\",\n",
    "\n",
    "\"Morgana\",\n",
    "\n",
    "\"Nami\",\n",
    "\n",
    "\"Nasus\",\n",
    "\n",
    "\"Nautilus\",\n",
    "\n",
    "\"Nidalee\",\n",
    "\n",
    "\"Nocturne\",\n",
    "\n",
    "\"Nunu\",\n",
    "\n",
    "\"Olaf\",\n",
    "\n",
    "\"Orianna\",\n",
    "\n",
    "\"Ornn\",\n",
    "\n",
    "\"Pantheon\",\n",
    "\n",
    "\"Poppy\",\n",
    "\n",
    "\"Quinn\",\n",
    "\n",
    "\"Rakan\",\n",
    "\n",
    "\"Rammus\",\n",
    "\n",
    "\"RekSai\",\n",
    "\n",
    "\"Renekton\",\n",
    "\n",
    "\"Rengar\",\n",
    "\n",
    "\"Riven\",\n",
    "\n",
    "\"Rumble\",\n",
    "\n",
    "\"Ryze\",\n",
    "\n",
    "\"Sejuani\",\n",
    "\n",
    "\"Shaco\",\n",
    "\n",
    "\"Shyvana\",\n",
    "\n",
    "\"Singed\",\n",
    "\n",
    "\"Skarner\",\n",
    "\n",
    "\"Sona\",\n",
    "\n",
    "\"Soraka\",\n",
    "\n",
    "\"Swain\",\n",
    "\n",
    "\"Syndra\",\n",
    "\n",
    "\"TahmKench\",\n",
    "\n",
    "\"Taliyah\",\n",
    "\n",
    "\"Talon\",\n",
    "\n",
    "\"Taric\",\n",
    "\n",
    "\"Thresh\",\n",
    "\n",
    "\"Tristana\",\n",
    "\n",
    "\"Trundle\",\n",
    "\n",
    "\"Tryndamere\",\n",
    "\n",
    "\"TwistedFate\",\n",
    "\n",
    "\"Twitch\",\n",
    "\n",
    "\"Udyr\",\n",
    "\n",
    "\"Urgot\",\n",
    "\n",
    "\"Varus\",\n",
    "\n",
    "\"Vayne\",\n",
    "\n",
    "\"Veigar\",\n",
    "\n",
    "\"Vi\",\n",
    "\n",
    "\"Viktor\",\n",
    "\n",
    "\"Vladimir\",\n",
    "\n",
    "\"Volibear\",\n",
    "\n",
    "\"Warwick\",\n",
    "\n",
    "\"Wukong\",\n",
    "\n",
    "\"Xayah\",\n",
    "\n",
    "\"Xerath\",\n",
    "\n",
    "\"XinZhao\",\n",
    "\n",
    "\"Yasuo\",\n",
    "\n",
    "\"Yorick\",\n",
    "\n",
    "\"Zed\",\n",
    "\n",
    "\"Ziggs\",\n",
    "\n",
    "\"Zilean\",\n",
    "\n",
    "\"Zoe\",\n",
    "\n",
    "\"Zyra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "mains_with_reddit = []\n",
    "for i in range(len(mains)):\n",
    "    mains_with_reddit.append(str(mains[i])+'mains')\n",
    "# mains_with_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of these mains is not like the others...\n",
    "# Most main subreddits are in the form *champname* + Mains\n",
    "# Some are not...here are the ones that are not in that format\n",
    "\n",
    "# Zac - r/thesecretweapon\n",
    "# Velkoz - r/velkoz \n",
    "# Teemo - r/TeemoTalk\n",
    "# Sivir - r/Sivir\n",
    "# Sion - r/DirtySionMains\n",
    "# Shen - r/Shen\n",
    "# Lux - r/Lux\n",
    "# Kindred - r/Kindred\n",
    "# Jinx - r/leagueofjinx \n",
    "# Janna - r/Janna\n",
    "# Illoia - r/Illaoi\n",
    "# Draven - r/Draven \n",
    "# A sol mains - r/Aurelion_Sol_mains -- already in list, kinda works\n",
    "\n",
    "main_sub_names = mains_with_reddit\n",
    "\n",
    "main_sub_names.append('thesecretweapon')\n",
    "main_sub_names.append('velkoz')\n",
    "main_sub_names.append('TeemoTalk')\n",
    "main_sub_names.append('Sivir')\n",
    "main_sub_names.append('DirtySionMains')\n",
    "main_sub_names.append('Shen')\n",
    "main_sub_names.append('Lux')\n",
    "main_sub_names.append('Kindred')\n",
    "main_sub_names.append('leagueofjinx')\n",
    "main_sub_names.append('Janna')\n",
    "main_sub_names.append('Illaoi')\n",
    "main_sub_names.append('Draven')\n",
    "main_sub_names.append('Alistar')\n",
    "\n",
    "# main_sub_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the scraping \n",
    "we have all the URL's we need! now the hard part...\n",
    "\n",
    "Try using PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slackerlol\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='F2XQbVxMX5Al6w',\n",
    "                     client_secret='GD0yT4BQvaeF--bzcJVCBCyrVA8',\n",
    "                     password='slack298l',\n",
    "                     user_agent='Lol Main subreddit scrapes by u/slackerlol',\n",
    "                     username='slackerlol')\n",
    "\n",
    "print(reddit.user.me()) #check if you got in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sion Q move\n",
      "113\n",
      "7an9go\n",
      "https://www.youtube.com/watch?v=LJYSjHm5up8\n",
      "Quality fuckin shit post, 5/7 my dude\n",
      "A masterpiece\n",
      "Nice\n",
      "i love this shit\n",
      "7/7 with rice.\n",
      "So you're a cultist if 5/7 too?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subreddit = reddit.subreddit('dirtysionmains')\n",
    "top_comments = []\n",
    "\n",
    "for submission in subreddit.top(limit=1):\n",
    "    print(submission.title)  # Output: the submission's title\n",
    "    print(submission.score)  # Output: the submission's score\n",
    "    print(submission.id)     # Output: the submission's ID\n",
    "    print(submission.url)    # Output: the URL the submission points to\n",
    "    \n",
    "#     top_level_comments = list(submission.comments)\n",
    "#     for top_level_comment in submission.comments:\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    for comment in submission.comments.list():\n",
    "        print(comment.body)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# assume you have a Reddit instance bound to variable `reddit`\n",
    "submission = reddit.submission(id='5vfjww')\n",
    "print(submission.title) # to make it non-lazy\n",
    "# pprint.pprint(vars(submission))\n",
    "(vars(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to return the metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def pull_sub_data(subreddit, lim):\n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit)\n",
    "    \n",
    "    titles = []\n",
    "    scores = []\n",
    "    post_ids = []\n",
    "    post_urls = []\n",
    "    comments_all = []\n",
    "    types = []\n",
    "    upvote_ratios = []\n",
    "    authors = []\n",
    "\n",
    "    for submission in subreddit.top(limit=lim):\n",
    "        title = submission.title  # Output: the submission's title\n",
    "        score = submission.score  # Output: the submission's score\n",
    "        post_id = submission.id   # Output: the submission's ID\n",
    "        post_url = submission.url   # Output: the URL the submission points to\n",
    "        author = submission.author \n",
    "        \n",
    "        # Append lists\n",
    "        titles.append(title)\n",
    "        scores.append(score)\n",
    "        post_ids.append(post_id)\n",
    "        post_urls.append(post_url)\n",
    "        authors.append(author)\n",
    "\n",
    "        # Get comments\n",
    "        comments = []\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            comment_app = {comment.author:comment.body} # Get author and content - in dict form\n",
    "            comments.append(comment_app)\n",
    "            \n",
    "        comments_all.append(comments)\n",
    "\n",
    "        # Get is_video and upvote ratio\n",
    "        upvote_ratio = vars(submission)['upvote_ratio']\n",
    "        upvote_ratios.append(upvote_ratio)\n",
    "        \n",
    "        try:\n",
    "            type_post = vars(submission)['media']['type']\n",
    "            types.append(type_post)\n",
    "        except:\n",
    "            if vars(submission)['is_reddit_media_domain'] == True:\n",
    "                types.append('reddit_media')\n",
    "            if vars(submission)['is_self'] == True:\n",
    "                types.append('self_post')\n",
    "            else:\n",
    "                types.append('unknown')\n",
    "        \n",
    "\n",
    "        \n",
    "    df = pd.DataFrame(list(zip(titles, authors, scores, post_ids, post_urls, comments_all, types,\n",
    "                              upvote_ratios)), columns=['title', 'author', 'score', 'post_id','post_url',\n",
    "                                                       'comments','type','upvote_ratio'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull_sub_data('dirtysionmains', 2)\n",
    "# sion_test = pull_sub_data('dirtysionmains', 2)\n",
    "# sion_test.to_csv('sion_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok!  So we got the first version of our text scrape written!\n",
    "Now to run through and do the actual scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 attempt for Aatroxmains failed; retrying...\n",
      "0 attempt for Elisemains failed; retrying...\n",
      "0 attempt for Singedmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n",
      "0 attempt for Tahm Kenchmains failed; retrying...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-9d7e6f5fd432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0msubreddit_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpull_sub_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchamp_main\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-144-2c792f5a8f69>\u001b[0m in \u001b[0;36mpull_sub_data\u001b[0;34m(subreddit, lim)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0msubmission\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubreddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m  \u001b[1;31m# Output: the submission's title\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python\\lib\\site-packages\\praw\\models\\listing\\generator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_list_index\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_listing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python\\lib\\site-packages\\praw\\models\\listing\\generator.py\u001b[0m in \u001b[0;36m_next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_listing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python\\lib\\site-packages\\praw\\reddit.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, path, params)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \"\"\"\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_objector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjectify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python\\lib\\site-packages\\praw\\reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, params, data, files)\u001b[0m\n\u001b[1;32m    471\u001b[0m         return self._core.request(method, path, data=data, files=files,\n\u001b[0;32m--> 472\u001b[0;31m                                   params=params)\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, data, files, json, params)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             params=params, url=url)\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Python\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[0;34m(self, data, files, json, method, params, url, retries)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'no_content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFound\u001b[0m: received 404 HTTP response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-9d7e6f5fd432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattempts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'attempt for'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchamp_main\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'failed; retrying...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "for champ_main in main_sub_names:\n",
    "    attempts = 0\n",
    "    # Sometimes we get a bad connection, so use try/except for 5 loops - if it fails after third\n",
    "    # then just pass and let it be known\n",
    "    while True and attempts <= 5:\n",
    "        try:\n",
    "            subreddit_df = pull_sub_data(champ_main, 50)\n",
    "        except:\n",
    "            print(attempts, 'attempt for', champ_main, 'failed; retrying...')\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    subreddit_df.to_csv(champ_main+'_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crashed on tkmains - restart from here manually \n",
    "main_sub_names.index('TahmKenchmains')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_sub_names = main_sub_names[95:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for champ_main in temp_sub_names:\n",
    "#     attempts = 0\n",
    "#     # Sometimes we get a bad connection, so use try/except for 5 loops - if it fails after third\n",
    "#     # then just pass and let it be known\n",
    "#     while True and attempts <= 5:\n",
    "#         try:\n",
    "#             subreddit_df = pull_sub_data(champ_main, 50)\n",
    "#         except:\n",
    "#             print(attempts, 'attempt for', champ_main, 'failed; retrying...')\n",
    "#             time.sleep(5)\n",
    "#             continue\n",
    "#         break\n",
    "\n",
    "#     subreddit_df.to_csv(champ_main+'_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
